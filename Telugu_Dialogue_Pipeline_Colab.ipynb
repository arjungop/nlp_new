{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af79fab5",
   "metadata": {},
   "source": [
    "# Telugu Multi-Turn Dialogue Pipeline\n",
    "### Optimised for Google Colab (T4 / A100 GPU)\n",
    "\n",
    "**Steps covered:**\n",
    "1. GPU check\n",
    "2. Mount Google Drive & upload project\n",
    "3. Install dependencies (with `faiss-gpu` + `bitsandbytes` for 4-bit quantisation)\n",
    "4. Run the full pipeline (data → embeddings → generation → evaluation → plots)\n",
    "5. Save outputs back to Drive\n",
    "\n",
    "> **Before running:** Change Runtime → **T4 GPU** (Runtime > Change runtime type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0527227d",
   "metadata": {},
   "source": [
    "## Step 1 — Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb363d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    print('⚠️  No GPU detected. Go to Runtime > Change runtime type and select T4 GPU.')\n",
    "    print('   The pipeline will still run on CPU but will be very slow.')\n",
    "else:\n",
    "    print(result.stdout)\n",
    "    print('✅ GPU is available!')\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch version : {torch.__version__}')\n",
    "print(f'CUDA available  : {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU device      : {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM            : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d7436",
   "metadata": {},
   "source": [
    "## Step 2 — Mount Google Drive & Set Up Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9fb33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# OPTION A: Project is already zipped in your Drive\n",
    "#   Upload nlp_new.zip to your Drive root, then run this block.\n",
    "#\n",
    "# OPTION B: Upload directly from local machine (next cell)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "DRIVE_ZIP = '/content/drive/MyDrive/nlp_new.zip'   # change if needed\n",
    "PROJECT_DIR = '/content/nlp_new'\n",
    "\n",
    "if os.path.exists(DRIVE_ZIP):\n",
    "    print(f'Found zip at {DRIVE_ZIP}. Extracting...')\n",
    "    !unzip -q \"{DRIVE_ZIP}\" -d /content/\n",
    "    print('✅ Extracted.')\n",
    "else:\n",
    "    print(f'❌ Zip not found at {DRIVE_ZIP}.')\n",
    "    print('   Run the next cell to upload your files directly.')\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f'Working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── OPTION B: Upload files directly from your local machine ──\n",
    "# Run this ONLY if you did not use Option A above.\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()   # select nlp_new.zip from your Mac\n",
    "# !unzip -q nlp_new.zip -d /content/\n",
    "# import os; os.chdir('/content/nlp_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838e2a4",
   "metadata": {},
   "source": [
    "## Step 3 — Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beacee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages.\n",
    "# faiss-gpu replaces faiss-cpu for CUDA acceleration.\n",
    "# bitsandbytes enables 4-bit quantisation of Gemma & Sarvam models.\n",
    "\n",
    "!pip install -q \\\n",
    "    pandas numpy \\\n",
    "    torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \\\n",
    "    transformers>=4.38.0 \\\n",
    "    accelerate \\\n",
    "    bitsandbytes \\\n",
    "    faiss-gpu \\\n",
    "    seaborn matplotlib \\\n",
    "    sentencepiece \\\n",
    "    huggingface_hub\n",
    "\n",
    "print('✅ All packages installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9409b4",
   "metadata": {},
   "source": [
    "## Step 4 — (Optional) Hugging Face Login\n",
    "Required for gated models like **google/gemma-2-2b-it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35225a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your Hugging Face token from https://huggingface.co/settings/tokens\n",
    "# Make sure you have accepted the Gemma model terms on the HF model page.\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()   # will prompt for token interactively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b7738",
   "metadata": {},
   "source": [
    "## Step 5 — Enable 4-bit Quantisation in Config\n",
    "This patches `config.py` to add the quantisation flag used by the model loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc808d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch model_t5.py and model_sarvam.py to use 4-bit quantisation.\n",
    "# This reduces VRAM usage from ~6 GB to ~2 GB per model.\n",
    "\n",
    "QUANT_PATCH = '''\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "'''\n",
    "\n",
    "import re, pathlib\n",
    "\n",
    "def patch_model_file(filepath: str) -> None:\n",
    "    src = pathlib.Path(filepath).read_text()\n",
    "    if 'BitsAndBytesConfig' in src:\n",
    "        print(f'{filepath} already patched.')\n",
    "        return\n",
    "\n",
    "    # Insert bnb_config block after the first 'from transformers import' line\n",
    "    src = re.sub(\n",
    "        r'(from transformers import .*\\n)',\n",
    "        r'\\1' + QUANT_PATCH,\n",
    "        src,\n",
    "        count=1\n",
    "    )\n",
    "\n",
    "    # Add quantization_config=bnb_config to from_pretrained calls\n",
    "    src = re.sub(\n",
    "        r'(AutoModelForSeq2SeqLM|AutoModelForCausalLM)\\.from_pretrained\\(([^)]+)\\)',\n",
    "        lambda m: m.group(0).rstrip(')') + ',\\n        quantization_config=bnb_config,\\n        device_map=\"auto\")',\n",
    "        src\n",
    "    )\n",
    "\n",
    "    pathlib.Path(filepath).write_text(src)\n",
    "    print(f'✅ Patched {filepath} with 4-bit quantisation.')\n",
    "\n",
    "patch_model_file('model_t5.py')\n",
    "patch_model_file('model_sarvam.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af31308",
   "metadata": {},
   "source": [
    "## Step 6 — Run the Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the dataset exists before starting\n",
    "import os\n",
    "\n",
    "TEL_TRAIN = 'IndicDialogue Dataset/dataset/Splitted_Dataset/train/tel/tel.jsonl'\n",
    "TEL_TEST  = 'IndicDialogue Dataset/dataset/Splitted_Dataset/test/tel/tel.jsonl'\n",
    "\n",
    "for p in [TEL_TRAIN, TEL_TEST]:\n",
    "    exists = os.path.exists(p)\n",
    "    status = '✅' if exists else '❌'\n",
    "    size   = f'({os.path.getsize(p)/1024:.1f} KB)' if exists else ''\n",
    "    print(f'{status} {p} {size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stage 1: Data Loading ──────────────────────────────────────\n",
    "import logging, sys\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(name)s %(levelname)s %(message)s',\n",
    "                    handlers=[logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "from config import Config\n",
    "from data_loader import DataLoader\n",
    "\n",
    "config = Config()\n",
    "loader = DataLoader(config)\n",
    "raw_df = loader.load_data(split='train')\n",
    "print(f'\\nLoaded {len(raw_df)} Telugu dialogue lines.')\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stage 2: Data Cleaning ────────────────────────────────────\n",
    "from data_cleaner import DataCleaner\n",
    "\n",
    "cleaner = DataCleaner(config)\n",
    "cleaned_df = cleaner.clean_dataset(raw_df, text_column='text')\n",
    "print(f'After cleaning: {len(cleaned_df)} rows (removed {len(raw_df)-len(cleaned_df)} noisy lines).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stage 3: Dialogue Segmentation ───────────────────────────\n",
    "from dialogue_segmenter import DialogueSegmenter\n",
    "\n",
    "segmenter = DialogueSegmenter(config)\n",
    "dialogue_pairs = segmenter.segment_dialogues(cleaned_df, text_column='text')\n",
    "print(f'Generated {len(dialogue_pairs)} dialogue pairs (context window = {config.context_window_size}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22fedcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stage 4: Triplet Construction ────────────────────────────\n",
    "from tripplet_builder import TripletBuilder\n",
    "\n",
    "builder  = TripletBuilder(config)\n",
    "triplets = builder.build_triplets(dialogue_pairs)\n",
    "print(f'Built {len(triplets)} anchor-positive triplets.')\n",
    "print('Sample triplet keys:', list(triplets[0].keys()) if triplets else 'N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stage 5: MuRIL Embedding (GPU-accelerated) ────────────────\n",
    "from embedder import MuRILEmbedder\n",
    "import torch\n",
    "\n",
    "print(f'Embedding on: {\"CUDA\" if torch.cuda.is_available() else \"CPU\"}')\n",
    "embedder = MuRILEmbedder(config)\n",
    "\n",
    "contexts = [str(t['anchor']) for t in triplets if t.get('anchor')]\n",
    "context_embeddings = embedder.get_embeddings(contexts)\n",
    "print(f'Embedding matrix shape: {context_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dfa9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stage 6: FAISS Vector DB ──────────────────────────────────\n",
    "from vectordb_store import VectorDBStore\n",
    "\n",
    "vector_db = VectorDBStore(config)\n",
    "vector_db.build_index(context_embeddings, triplets)\n",
    "vector_db.save_index()\n",
    "print('✅ FAISS index built and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stage 7: Response Generation (4-bit quantised models) ─────\n",
    "# This is the slowest stage. Checkpointing is built-in — if it crashes,\n",
    "# re-run this cell and it will resume from the last checkpoint.\n",
    "\n",
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from response_generator import ResponseGenerator\n",
    "\n",
    "generator = ResponseGenerator(config)\n",
    "enriched_triplets = generator.generate_all_responses(triplets)\n",
    "print(f'✅ Generated responses for {len(enriched_triplets)} triplets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stage 8: Evaluation ───────────────────────────────────────\n",
    "from evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator(config)\n",
    "evaluation_results = evaluator.evaluate_dataset(enriched_triplets)\n",
    "print(f'✅ Evaluated {len(evaluation_results)} samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb803c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stage 9: Metrics Logging ──────────────────────────────────\n",
    "from metrics_logger import MetricsLogger\n",
    "\n",
    "metrics_logger = MetricsLogger(config)\n",
    "metrics_logger.log_results(evaluation_results)\n",
    "print('✅ Metrics saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d415f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Stage 10: Results Analysis & Visualisation ────────────────\n",
    "from results_analyzer import ResultsAnalyzer\n",
    "from visualizer import Visualizer\n",
    "\n",
    "analyzer = ResultsAnalyzer(config)\n",
    "summary_matrix = analyzer.analyze_metrics(evaluation_results)\n",
    "analyzer.print_summary(summary_matrix)\n",
    "\n",
    "visualizer = Visualizer(config)\n",
    "visualizer.plot_heatmap(summary_matrix)\n",
    "visualizer.plot_bar_charts(summary_matrix)\n",
    "\n",
    "print('\\n✅ Pipeline completed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd0df22",
   "metadata": {},
   "source": [
    "## Step 7 — Save Outputs to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "DRIVE_OUTPUT = '/content/drive/MyDrive/nlp_new_outputs'\n",
    "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
    "\n",
    "# Copy outputs, logs, and vector_db to Drive\n",
    "for folder in ['outputs', 'logs', 'vector_db']:\n",
    "    src = os.path.join('/content/nlp_new', folder)\n",
    "    dst = os.path.join(DRIVE_OUTPUT, folder)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "        print(f'✅ Copied {folder}/ → Drive')\n",
    "    else:\n",
    "        print(f'⚠️  {folder}/ not found, skipping.')\n",
    "\n",
    "print(f'\\nAll outputs saved to {DRIVE_OUTPUT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c596c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: also zip and download to your local machine\n",
    "# Uncomment to run.\n",
    "\n",
    "# import shutil\n",
    "# from google.colab import files\n",
    "# shutil.make_archive('/content/nlp_new_outputs', 'zip', DRIVE_OUTPUT)\n",
    "# files.download('/content/nlp_new_outputs.zip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
